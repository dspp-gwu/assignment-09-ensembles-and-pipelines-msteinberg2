{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods and Pipelines\n",
    "\n",
    "We could spend an entire course on ensemble methods.  Ensembles combine more than one model in some systematic way. In practice, data scientists do this because it helps them predict outcomes with more accuracy and with less bias than any single method does.  Because there are thousands of possible algorithms that can be combined in what are effectively limitless ways, there is no way to cover all of ensemble methods.  Instead, we will discuss one particular ensemble method, RANDOM FORESTS, that is very widely used.  You should be aware, however, that the idea of combining multiple relatively \"weak\" machine learning methods into a single \"strong\" machine learning method is quite common, and that most sophisticated machine learning models do this. \n",
    "\n",
    "Also in this assignment, we will introduce the idea of a \"pipeline\", a collection of steps that you can use to automate much of your data science process so that testing modifications and variations is relatively simple and straightforward.  Pipelines require a little getting used to, but they are worth the effort. \n",
    "\n",
    "Rather than run through a bunch of explanations, we're going to build an ensemble model, then talk about what it does. Then we'll do the same for a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first let's read in the Titanic data \n",
    "df_train = pd.read_csv('train.csv', header = 0)\n",
    "df_test = pd.read_csv('test.csv', header = 0)\n",
    "df = pd.concat([df_train, df_test], keys=[\"train\", \"test\"])\n",
    "\n",
    "# then a little feature engineering\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Child'] = df['Age'] < 14\n",
    "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "\n",
    "df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())  # titles might mean something\n",
    "df['Title'][df.Title == 'Jonkheer'] = 'Master'\n",
    "df['Title'][df.Title.isin(['Ms','Mlle'])] = 'Miss'\n",
    "df['Title'][df.Title == 'Mme'] = 'Mrs'\n",
    "df['Title'][df.Title.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'\n",
    "df['Title'][df.Title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'\n",
    "\n",
    "df['NameLength'] = df.Name.apply(lambda x: len(x))   # fancy titles are longer\n",
    "df = pd.get_dummies(data=df, columns=['Pclass','Sex','Embarked','Title'], drop_first=False).copy()  # make dummies for categories\n",
    "df = df.drop(['Name','Cabin','Ticket'], axis=1).copy().dropna()  # drop the categorical columns\n",
    "data = df.loc['train'] \n",
    "features = data.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 78.1\n",
      "Precision: 78.6\n",
      "Precision: 81.5\n",
      "Precision: 79.0\n",
      "Precision: 74.6\n"
     ]
    }
   ],
   "source": [
    "# and then cross validation just like the last class with the rat data\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for train_index, test_index in cv.split(data):\n",
    "    X_train = data.loc[train_index].drop(['Survived'], axis=1)\n",
    "    y_train = data.loc[train_index]['Survived']\n",
    "    X_test = data.loc[test_index].drop(['Survived'], axis=1)\n",
    "    y_test = data.loc[test_index]['Survived']\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    predicted = clf.predict(X_test)\n",
    "    print('Precision: '+str(100 * round(precision_score(y_test, predicted),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifiers\n",
    "\n",
    "Now copy that last block of code, and where you see \n",
    "\n",
    "```clf = LogisticRegression() ```\n",
    "\n",
    "replace it with \n",
    "\n",
    "```clf = RandomForestClassifier()```\n",
    "\n",
    "Compare your precision scores.  How do they compare? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 84.9\n",
      "Precision: 83.6\n",
      "Precision: 83.9\n",
      "Precision: 79.6\n",
      "Precision: 78.0\n"
     ]
    }
   ],
   "source": [
    "# And now let's do the exact same thing, but let's use a Random Forest Classifier\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for train_index, test_index in cv.split(data):\n",
    "    X_train = data.loc[train_index].drop(['Survived'], axis=1)\n",
    "    y_train = data.loc[train_index]['Survived']\n",
    "    X_test = data.loc[test_index].drop(['Survived'], axis=1)\n",
    "    y_test = data.loc[test_index]['Survived']\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    predicted = clf.predict(X_test)\n",
    "    print('Precision: '+str(100 * round(precision_score(y_test, predicted),3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We changed the model we used from LogisticRegression to RandomForestClassifier. What is that, and why did that change increase the average precision?  \n",
    "\n",
    "A logistic regression is an example of a linear model, which assumes that the relationship between each feature and the output class is a linear one. For example, if one of the features is the depth of a well, a linear model will assume that (all other things being equal) the difference in functionality between a 10-year-old and a 50-year-old  will be the same as the difference between 40-year-old and 90-year-old. This isn’t always a valid assumption. One way to address it is to add extra features like age squared and the logarithm of age, which helps a linear model capture nonlinearities, but might not still allow me to get all the nuances of nonlinear relationships.A logistic regression also doesn’t capture interactions between features. I can explicitly add interaction terms to the logistic regression, but this gets unwieldy fast when I have many features.\n",
    "\n",
    "A decision tree can capture interactions and nonlinearities much more naturally than logistic regression, because of the binary tree structure of the decision tree algorithm itself. The downside of decision trees is that they can be harder to interpret or assign uncertainties to their predictions.\n",
    "\n",
    "A random forest is a collection of decision trees, each of which is trained on a subset of the rows/columns of the training data. The randomness in the training set means that the individual trees in a random forest are high-variance, but low-bias, and the final prediction is made by having each tree classify a given event and then using their predictions as “votes,” with the majority opinion being assigned as the label. Nonlinearities and interactions are captured by the individual trees, but ensembling many trees into a random forest tends to cancel out the biases/shortcomings of any one tree producing a stronger predictor overall.\n",
    "\n",
    "In empirical studies of many algorithms being applied to many supervised learning problems, random forests often come out on top overall. So when in doubt, or if I only have the time/resources to try one model, a random forest is likely to get at or near the peak performance of all the algorithms on the market.\n",
    "\n",
    "My favorite explanation of random forests involves actual forests.  Have a look here: http://whrc.org/wp-content/uploads/2016/02/DecisionTrees_RandomForest_v2.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest components\n",
    "\n",
    "Random forests do not have a single canonical form.  Sci-Kit Learn has great defaults, but under the covers there are some important parameters that you may need to tune.  These include \"max_features\", \n",
    "\n",
    "\n",
    "### max_features\n",
    "\n",
    "max_features sets the maximum number of features in an individual tree and is probably the most important parameter. The more features allowed, the more computations (and thus time) required. Also, the greater chance of overfitting.  Here are a few standard options for this parameter:\n",
    "\n",
    "- Auto/None : This will simply take all the features which make sense in every tree.Here we simply do not put any restrictions on the individual tree.\n",
    "- sqrt : This option will take square root of the total number of features in individual run. For instance, if the total number of variables are 100, we can only take 10 of them in individual tree.”log2″ is another similar type of option for max_features.\n",
    "- 0.2 : This option allows the random forest to take 20% of variables in individual run. We can assign and value in a format “0.x” where we want x% of features to be considered.\n",
    "\n",
    "\n",
    "### max_depth\n",
    "\n",
    "max_depth sets the maximum  depth of the tree. By default, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.  Because tree depth can be quite deep with lots of features, this adds another way to reduce overfitting and increase speed. \n",
    "\n",
    "\n",
    "### n_estimators \n",
    "\n",
    "n_estimators sets the number of trees built before each \"weak learning\" tree votes. More trees means more computation and thus more time. But this does not risk overfitting. You should choose as high value as your processor can handle because this makes your predictions stronger and more stable. sklearn defaults to ten.  \n",
    "\n",
    "\n",
    "### min_sample_leaf\n",
    "\n",
    "min_sample_leave (minimum sample leaf size) sets how many outcome votes have to exist for a leaf to have any way.  Smaller leaf makes the model more prone to overfitting. But if you don't have much data, it's best to start with the default (1).  If you think you are overfitting, you can increase that number.\n",
    "\n",
    "### oob_score\\\n",
    "Finally, we can use a kind of internal cross validation special to random forests to estimate the generalization accuracy. This really helps reduce overfitting, but also cuts out potentially important edge cases your classifier might identify. \n",
    "\n",
    "To set a parameter, you just add it inside the parentheses after the model name.  Like so: \n",
    "\n",
    "```     clf = RandomForestClassifier(random_state=1, min_sample_leaf=2) ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# excise 01 - Adjust Parameters\n",
    "\n",
    "Re-run our random forest example from above, but this time set max_features to 20 and oob_score to 'True'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 02 -- try  with your data\n",
    "\n",
    "Now, try using the RandomForest classifier (if your dependent variable is binary or categorical) or regressor (if it is continuous) on your own data.  Start with the defaults (no parameter settings), then tweak some of them and see how they affect your model's speed and performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Pipelines help you automate some of your work and make it a little more systematic.  As you can probably tell, even though you aren't a software engineer, machine learning is all about letting machines do the work of learning.  \n",
    "\n",
    "For example, we could add or subtract features, and see what happens, just like the old-timey statistians used to do.  But what if we have hudreds or thousands to choose from? And, if we decide to use random forests or some other modern ML model, we could tweak the parameters, like number of featurs, and max tree depth. But there are so many! \n",
    "\n",
    "Wouldn't it be much easiers to just let the computer do that? Plus, if we give the computer some generic standards, we can take our own biases out of feature selection and parameter tuning.  \n",
    "\n",
    "That's what pipelines allow you to do.  You build a pipeline, and then tell the computer to use that pipeline to run lots of combinations of different features and parameters, and tell you what works best.  Let's try it out quickly, then explore a bit after that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "A pipeline just takes some of the steps in your model-builng process and automates it.  We'll build a simple one here, but they can get quite coplex.  In fact, the more complex your process, the more you'll probably want to use pipelines. \n",
    "\n",
    "Pipelines can do two things: transform data (tyically for feature engineering) and estimate with the data (predicting outcomes given some data).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.75      0.90      0.82       128\n",
      "        1.0       0.81      0.60      0.69        95\n",
      "\n",
      "avg / total       0.78      0.77      0.76       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.pipeline\n",
    "X = data.drop(['Survived'], axis=1).as_matrix()\n",
    "y = data['Survived'].as_matrix()\n",
    "\n",
    "select = sklearn.feature_selection.SelectKBest(k='all')\n",
    "clf = sklearn.ensemble.RandomForestClassifier(random_state=1)\n",
    "\n",
    "steps = [('feature_selection', select),\n",
    "         ('random_forest', clf)]\n",
    "\n",
    "pipeline = sklearn.pipeline.Pipeline(steps) \n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
    "\n",
    "pipeline.fit( X_train, y_train ) # fit your pipeline on X_train and y_train\n",
    "y_prediction = pipeline.predict( X_test ) # call pipeline.predict() on your X_test data to make a set of test predictions\n",
    "report = sklearn.metrics.classification_report( y_test, y_prediction ) # test your predictions using sklearn.classification_report()\n",
    "print(report) # and print the report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search & Parameter Tuning\n",
    "\n",
    "But where pipelines really shine is tuning hyperparameters.  You can do this with for loops, but pipelines make it much easiers.  \n",
    "\n",
    "For exmple, with Random Forest Classification, we might want to tinker with parameters like the number of estimators, or the minimum number of samples.  Each of this is settable by using sklearn's pipeline and grid search tools.  The only trick is that we set these by using the model name, then two underscores, then the parameter name.   So instead of using\n",
    "\n",
    "```\n",
    "random_forest.n_estimators\n",
    "```\n",
    "\n",
    "you should use \n",
    "\n",
    "```\n",
    "random_forest__n_estimators\n",
    "```\n",
    "\n",
    "We follow that with a list of the values we want to try.  So, if I wanted to try all the values between 5 and 10 I could use either: \n",
    "\n",
    "``` \n",
    "random_forest__n_estimators=[5,6,7,8,9, 10]\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "``` \n",
    "random_forest__n_estimators=list(range(5,11))\n",
    "```\n",
    "\n",
    "which produces the same thing.  If I think that trying every value will take too long (remember, every additional variation is multiplied by all the other variations!), then maybe just try a few.  \n",
    "\n",
    "So here is how we search over a large number of possible parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.92      0.84       128\n",
      "        1.0       0.86      0.63      0.73        95\n",
      "\n",
      "avg / total       0.81      0.80      0.79       223\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1080 out of 1080 | elapsed:   23.1s finished\n"
     ]
    }
   ],
   "source": [
    "import sklearn.grid_search\n",
    "\n",
    "parameters = dict(feature_selection__k=[5, 7, 9, 11, 13, 15, 17, 19, 20], # listing each one\n",
    "              random_forest__n_estimators=list(range(5,21,2)),  # generating a list up, skipping by two\n",
    "              random_forest__min_samples_split=list(range(2,11,2))) \n",
    "\n",
    "cv = sklearn.grid_search.GridSearchCV(pipeline, param_grid=parameters, verbose=True)\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "y_predictions = cv.predict(X_test)\n",
    "report = sklearn.metrics.classification_report(y_test, y_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our pipeline tried 1080 possible parameter combinations, and it fit across all of them.  Sure, you could do that by hand, but ... why would you?  Pipelines are great for this kind of extremely tedious work. Whenever possible, just let the computer do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 03 - try with your own data\n",
    "\n",
    "Now try a grid search over a few parameters using your own data.  Start small -- it can take a while!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding your horizons\n",
    "\n",
    "We don't have time to cover all the possible classifiers you can possibly use, but in the code that follows, we run through a few that you might find useful: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.653631284916\n",
      "Nearest Neighbors 0.662921348315\n",
      "Nearest Neighbors 0.629213483146\n",
      "Nearest Neighbors 0.634831460674\n",
      "Nearest Neighbors 0.61797752809\n",
      "Linear SVM 0.59217877095\n",
      "Linear SVM 0.556179775281\n",
      "Linear SVM 0.606741573034\n",
      "Linear SVM 0.623595505618\n",
      "Linear SVM 0.64606741573\n",
      "Gaussian Process 0.597765363128\n",
      "Gaussian Process 0.601123595506\n",
      "Gaussian Process 0.550561797753\n",
      "Gaussian Process 0.573033707865\n",
      "Gaussian Process 0.634831460674\n",
      "Decision Tree 0.776536312849\n",
      "Decision Tree 0.752808988764\n",
      "Decision Tree 0.831460674157\n",
      "Decision Tree 0.825842696629\n",
      "Decision Tree 0.831460674157\n",
      "Random Forest 0.765363128492\n",
      "Random Forest 0.792134831461\n",
      "Random Forest 0.842696629213\n",
      "Random Forest 0.859550561798\n",
      "Random Forest 0.831460674157\n",
      "Neural Net 0.776536312849\n",
      "Neural Net 0.769662921348\n",
      "Neural Net 0.589887640449\n",
      "Neural Net 0.674157303371\n",
      "Neural Net 0.842696629213\n",
      "AdaBoost 0.776536312849\n",
      "AdaBoost 0.792134831461\n",
      "AdaBoost 0.820224719101\n",
      "AdaBoost 0.820224719101\n",
      "AdaBoost 0.820224719101\n",
      "Naive Bayes 0.754189944134\n",
      "Naive Bayes 0.76404494382\n",
      "Naive Bayes 0.797752808989\n",
      "Naive Bayes 0.803370786517\n",
      "Naive Bayes 0.814606741573\n",
      "QDA 0.40782122905\n",
      "QDA 0.432584269663\n",
      "QDA 0.387640449438\n",
      "QDA 0.348314606742\n",
      "QDA 0.342696629213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(),\n",
    "    GaussianProcessClassifier(),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    for train_index, test_index in cv.split(data):\n",
    "        X_train = data.loc[train_index].drop(['Survived'], axis=1)\n",
    "        y_train = data.loc[train_index]['Survived']\n",
    "        X_test = data.loc[test_index].drop(['Survived'], axis=1)\n",
    "        y_test = data.loc[test_index]['Survived']\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 04 - try with your own data\n",
    "\n",
    "Try another algorithm with your own data. Sklean makes it easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
